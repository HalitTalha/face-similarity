{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import os\n",
    "from model.densenet import DenseNet\n",
    "from src.pre_processing import *\n",
    "import matplotlib.pyplot as plt\n",
    "from src.contrastive import contrastive_loss\n",
    "import json\n",
    "from src.utils import Dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = './tboard_logs'\n",
    "model_id = 31911\n",
    "test_dataset_path = './dataset/test_v2.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(work_dir, str(model_id))\n",
    "\n",
    "# load training metadata (setup path if necessary)\n",
    "with open(checkpoint_dir + '/meta.json', 'r') as fp:\n",
    "    training_args = Dotdict(json.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager\n",
    "test_filenames = [test_dataset_path]\n",
    "test_dataset = tf.data.TFRecordDataset(test_filenames)\n",
    "test_dataset = test_dataset.map(tf_record_parser)\n",
    "test_dataset = test_dataset.map(random_resize_and_crop)\n",
    "test_dataset = test_dataset.map(normalizer)\n",
    "test_dataset = test_dataset.shuffle(1000)\n",
    "test_dataset = test_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"k\": training_args.growth_rate,\n",
    "        \"weight_decay\": training_args.l2_regularization,\n",
    "        \"num_outputs\": training_args.num_outputs,\n",
    "        \"units_per_block\": training_args.units_per_block,\n",
    "        \"momentum\": training_args.momentum,\n",
    "        \"epsilon\": training_args.epsilon,\n",
    "        \"initial_pool\": training_args.initial_pool}\n",
    "\n",
    "model = DenseNet(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tfe.Checkpoint(model=model,\n",
    "                      optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "try:\n",
    "    root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    print(\"Model {} successfully loaded.\".format(model_id))\n",
    "except:\n",
    "    print(\"Error loading model: {}\".format(FLAGS.model_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_similarity = []\n",
    "mean_dissimilarity = []\n",
    "\n",
    "for (batch, (Xi, Xj, label)) in enumerate(test_dataset):\n",
    "\n",
    "    with tf.contrib.summary.record_summaries_every_n_global_steps(100):\n",
    "\n",
    "        GX1 = model(Xi, training=False)\n",
    "        GX2 = model(Xj, training=False)\n",
    "        _, Dw = contrastive_loss(GX1, GX2, label, margin=2.)\n",
    "\n",
    "        f, axarr = plt.subplots(2, 8, figsize=(16,4))\n",
    "        f.subplots_adjust(hspace=0.3)\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "\n",
    "            Si = denormalize(Xi[i]).numpy()\n",
    "            Sj = denormalize(Xj[i]).numpy()\n",
    "\n",
    "            if label[i].numpy() == 0:\n",
    "                mean_similarity.append(Dw[i])\n",
    "            else:\n",
    "                mean_dissimilarity.append(Dw[i])\n",
    "\n",
    "            axarr[0, i].set_title('Sim: ' + str(Dw[i].numpy()))\n",
    "            axarr[0,i].imshow(np.squeeze(Si))\n",
    "            axarr[0,i].set_axis_off()\n",
    "\n",
    "            axarr[1,i].set_title(\"Label: \" + str(label[i].numpy()))\n",
    "            axarr[1,i].imshow(np.squeeze(Sj))\n",
    "            axarr[1,i].set_axis_off()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "mean_std_similarity_np = np.std(mean_similarity)\n",
    "mean_std_dissimilarity_np = np.std(mean_dissimilarity)\n",
    "mean_similarity_np = np.mean(mean_similarity)\n",
    "mean_dissimilarity_np = np.mean(mean_dissimilarity)\n",
    "\n",
    "print(\"Mean similarity {0} Mean Std: {1}.\".format(mean_similarity_np, mean_std_similarity_np))\n",
    "print(\"Mean dissimilarity {0} Mean Std: {1}.\".format(mean_dissimilarity_np, mean_std_dissimilarity_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
